{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 WELCOME TO LANGCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.0 LLMs and Chat Models\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# string 기반 질문\n",
    "chat.predict(\"How many planets are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Predict Messages\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# message 기반 질문\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"너는 지리학자야\"\n",
    "    ),\n",
    "    AIMessage(\n",
    "        content=\"안녕\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"멕시코와 태국 사이의 거리가 얼마나돼? 그리고 너의 이름은 뭐야?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "chat.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2 prompt template\n",
    "# 프롬프트 사용\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# string -> prompt -> LLM\n",
    "template = PromptTemplate.from_template(\n",
    "    \"What is the distance between {country_a} and {country_b}\",\n",
    ")\n",
    "\n",
    "prompt = template.format(country_a=\"Mexico\", country_b=\"Thailand\")\n",
    "\n",
    "\n",
    "chat.predict(prompt)\n",
    "\n",
    "# message -> prompt -> LLM, 객체가 아닌 string으로 system, ai, human 구분\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a geography expert. And you only reply in {language}.\"),\n",
    "        (\"ai\", \"Ciao, mi chiamo {name}!\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"What is the distance between {country_a} and {country_b}. Also, what is your name?\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    language=\"Greek\",\n",
    "    name=\"Socrates\",\n",
    "    country_a=\"Mexico\",\n",
    "    country_b=\"Thailand\",\n",
    ")\n",
    "\n",
    "\n",
    "chat.predict_messages(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.3 OutputParser and LCEL\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "# BaseOutputParser의 필수 정의 메소드 parse 정의\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    \n",
    "# 템플릿 정의\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase.Do NOT reply with anything else.\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 랭체인 표현 언어 | 사용: chain 을 통해, 수행 관계를 보다 간결하게 표현\n",
    "# CommaOutputParser.parse -> chat.predict_messages -> template(=prompt from messages)\n",
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"max_items\": 5, \n",
    "    \"question\": \"What are the pokemons?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.4 Chaining Chains ~ #3.5 Recap\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# streaming을 기반으로 답변이 한 글자 단위로 추가되고\n",
    "# callbacks을 통해 답변의 생성 event를 감지해서 한글자 단위로 print하는 동작을 수행한다.\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 첫번째 chain\n",
    "chef_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\",\n",
    "        ),\n",
    "        (\"human\", \"I want to cook {cuisine} food.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chef_chain = chef_prompt | chat\n",
    "\n",
    "# 두번째 chain\n",
    "veg_chef_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a vegetarian chef specialized on making traditional recipies vegetarian. You find alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative for a food just say you don't know how to replace it.\",\n",
    "        ),\n",
    "        (\"human\", \"{recipe}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "veg_chain = veg_chef_prompt | chat\n",
    "\n",
    "# chain 결합 후 사용\n",
    "# RunnableMap 개념 사용\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "\n",
    "final_chain.invoke({\"cuisine\": \"indian\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 MODEL IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.1 FewShotPromptTemplate\n",
    "# 예시를 제공함으로써 LLM의 이해를 돕는 방식\n",
    "# prompt 구성에 예시와 질문이 동일한 계층에서 순서만 다르게 입력된다.\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# key를 미리 정의해 놓은 example\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# exsample이 key로 활용할 template 형식 정의\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "# exsample prompt에 examples의 key들을 차례로 주입하여 FewShot 형성\n",
    "# suffix을 통해 질문받을 템플릿 구성\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\"country\": \"Turkey\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2 FewShotChatMessagePromptTemplate\n",
    "# message를 활용하여 system에게 역할 부여 가능\n",
    "# suffix 를 사용하지 않음(?)\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "        (\"ai\", \"{answer}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a geography expert, you give short answers.\"),\n",
    "        example_prompt,\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | chat\n",
    "\n",
    "chain.invoke({\"country\": \"Thailand\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.3 LengthBasedExampleSelector\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import example_selector\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# BaseExampleSelector을 사용해서 사용자 정의 selector를 정의\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "# 특정 로직에 따라 examples중에 select하는 과정 수행\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Brazil\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.4 Serialization and Composition\n",
    "# Serialization이란, 디스크에서 prompt templates를 가져오고 저장하는 방식을 의미 <- 프롬프트 엔지니어와 개발자 사이의 업무 교류를 위해 사용\n",
    "\n",
    "# 프롬프트 불러오기\n",
    "# from langchain.prompts import load_prompt\n",
    "# prompt.format(country=\"xxx\")\n",
    "\n",
    "# 프롬프트의 memory를 전부 모으는 방법\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# prompt 들 3개 정의\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "\n",
    "    {example}\n",
    "    \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# final와 프롬프트 3개의 관계 설정\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "\n",
    "# 다중 프롬프트를 final 프롬프트 형식에 맞춰서 종합\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    ")\n",
    "\n",
    "\n",
    "chain = full_prompt | chat\n",
    "\n",
    "# 다중 템플릿의 모든 key를 한번에 정의 가능\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"character\": \"Pirate\",\n",
    "        \"example_question\": \"What is your location?\",\n",
    "        \"example_answer\": \"Arrrrg! That is a secret!! Arg arg!!\",\n",
    "        \"question\": \"What is your fav food?\",\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.5 Caching\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "# LLM으로 보내는 요청의 횟수를 줄임으로써 절약 수행\n",
    "\n",
    "# 메모리에 캐싱\n",
    "# set_llm_cache(InMemoryCache())\n",
    "\n",
    "# DB에 캐싱\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "\n",
    "# 현재 진행중인 프로세스에 대해 동적으로 출력 수행\n",
    "# set_debug(True)\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler(),\n",
    "    # ],\n",
    ")\n",
    "\n",
    "# 프롬프트 -> LLM\n",
    "chat.predict(\"How do you make italian pasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전에 받았던 것 반환\n",
    "chat.predict(\"How do you make italian pasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.6 Serialization\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "chat = OpenAI(temperature=0.1, max_tokens=450, model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "# LLM 모델 설정 저장\n",
    "chat.save(\"model.json\")\n",
    "# LLM 모델 설정 불러오기\n",
    "loaded_chat = load_llm(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 사용료 미리 특정\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"What is the recipe for soju\")\n",
    "    b = chat.predict(\"What is the recipe for bread\")\n",
    "    print(a, \"\\n\")\n",
    "    print(b, \"\\n\")\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5 MEMORY\n",
    "- 랭체인은 5가지의 메모리가 존재\n",
    "- 메모리를 추가하지 않으면 챗봇은 아무것도 기억하지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.0 ConversationBufferMemory\n",
    "# 대화 내용을 전부 기억하는 메모리. 단, 대화가 길어질수록 메모리 사용 비효율적\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# return_messages=True은 챗모델에서만 설정\n",
    "# return_messages 설정은 messages 저장 방식을 설정(class or plain text)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# memory 종류과 관계없이 API 동일\n",
    "memory.save_context({\"input\":\"Hi!\"}, {\"output\":\"How are you\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.1 ConversationBufferWindowMemory\n",
    "# 대화의 특정 부분만을 저장하는 메모리. 최근 대화에만 집중\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4,\n",
    ")\n",
    "\n",
    "memory.save_context({\"input\":\"Hi!\"}, {\"output\":\"How are you\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.2 ConversationSummaryMemory\n",
    "# 대화를 요약해서 저장해놓는 메모리. 메모리 관리에 비용이 발생하지만 대화가 길어질 수록 입력 토큰 요구량이 효율적으로 관리됨.\n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# 메모리를 실행하는데에 비용이 든다는 걸 의미\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "add_message(\"South Kddorea is so pretty\", \"I wish I could go!!!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.3 ConversationSummaryBufferMemory\n",
    "# 메모리에 보내온 메시지의 수를 저장. limit에 다다른 순간에 오래된 메시지들을 요약하여 저장\n",
    "# 버퍼 메모리와 요약 메모리가 결합된 메모리에 해당.\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# max_token_limit은 메시지들이 요약되기 전의 가능한 메시지 토큰 수의 최대값.\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=80,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"South Korea is so pretty\", \"I wish I could go!!!\")\n",
    "add_message(\"How far is Korea from Argentina?\", \"I don't know! Super far!\")\n",
    "add_message(\"How far is Brazil from Argentina?\", \"I don't know! Super far!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.4 ConversationKGMemory\n",
    "# llm 사용 메모리. 대화 중의 엔티티 knowledge graph 생성. 가장 중요한 것들만 뽑아내는 요약본과 같은 기능 수행\n",
    "\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "# input을 통해 메모리에서 원하는 정보를 LLM으로 정리해서 추출한다.\n",
    "memory.load_memory_variables({\"input\": \"who is Nicolas\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Nicolas likes kimchi\", \"Wow that is so cool!\")\n",
    "memory.load_memory_variables({\"inputs\": \"what does nicolas like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.5 Memory on LLMChain\n",
    "# 메모리를 chain에 꽂는 방법과 두 종류의 chain을 사용해서 꽂는 방법\n",
    "# LLMchain은 off-the-shelf(일반적인 목적을 가진) chain을 의미한다.\n",
    "\n",
    "# langchain expression 언어를 활용해서 만든 chain은 프레임워크를 다루느라 머리 싸매거나 off-the-shelf chain을 커스텀하기보다 직접 만들고 싶을 때 사용\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# 프롬프트 템플릿에 memory를 위한 공간을 만들고\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "# 사용중에 있는 memory class에게 history를 넣을 곳을 알려준다.\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=10,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")\n",
    "chain.predict(question=\"I live in Seoul\")\n",
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.6 Chat Based Memory\n",
    "# memory 클래스를 plain이 아닌 class로 출력할 경우\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# return_messages=True\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "# MessagesPlaceholder을 사용해서 누가보냈는지 모를 문맥 context를 추가할 자리를 prompt에 만든다.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")\n",
    "chain.predict(question=\"I live in Seoul\")\n",
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.7 LCEL Based Memory\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    # memory history 불러오기\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "# RunnablePassthrough은 prompt가 format되기 전에 우리가 함수를 실행시키는걸 허락해준다.\n",
    "# RunnablePassthrough 단계에서 {\"question\": question} 와 {\"history\": load_memory의 반환값}을 결합하여 prompt으로 넘겨준다.\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "# invoke 동작에 대한 메모리 저장 수행\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    # memory history 저장하기\n",
    "    memory.save_context(\n",
    "        # Human\n",
    "        {\"input\": question},\n",
    "        # AI\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "# \n",
    "invoke_chain(\"My name is nico\")\n",
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.8 Recap\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "\n",
    "invoke_chain(\"My name is nico\")\n",
    "invoke_chain(\"What is my name?\")\n",
    "\n",
    "\"\"\"\n",
    "강의 내용\n",
    "\n",
    "- off-the-shelf Chain: 자동으로 LLM으로부터 응답값을 가져오고 memory를 업데이트하는걸 의미\n",
    "\n",
    "프롬프트에 메모리 추가 방법\n",
    "1. LLM chain 활용\n",
    "2. Chat prompt template 활용\n",
    "3. 수동 메모리 관리 방식\n",
    "-> 3번 \"수동 메모리 관리 방식\"을 가장 추천하심\n",
    "- 메모리를 저장한 후에 메모리로 할 수 있는 것들이 존재함(다른 곳에서 메모리 호출 가능, 데이터베이스를 가지고 있다면 다른 곳에서 데이터베이스 호출 가능)\n",
    "- LLM이 가져오는 프롬프트에 기록들을 넣는건 \"우리의 일\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#6 RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Data Loaders and Splitters\n",
    "# Data Loader 종류는 매우 많다. 피그마, 페이스북, 슬랙, 파워포인트, ...\n",
    "# UnstructuredFileLoader을 사용할 경우 다양한 파일을 읽어올 수 있다.\n",
    "# load_and_split을 사용해서 splitter에 의한 chunk 나누기 수행 가능\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 고려한 점: chunk를 나눌 때, chunk 내의 문맥이 파괴되진 않았는지, chunk size 는 어떤지 , chunk 의 문맥 유지를 위한 overlap 은 얼마나 필요한지, cunk 구분점을 위한 separator \"\\n\" 는 어떻게 설정하는게 좋은지 고려하는 과정이 필요하다\n",
    "splitter = RecursiveCharacterTextSplitter()\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "loader.load_and_split(text_splitter=splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.2 Tiktoken (04:19)\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 텍스트 길이를 계산할 때, model과 같은 방법으로 계산하는 게 좋다. 이때, model은 토큰을 기준으로 연산하므로 이를 고려해야 한다.\n",
    "# from_tiktoken_encoder을 사용하여, model의 토큰 개수 계산을 고려하여 chunk를 나누는 작업을 수행한다.\n",
    "# tiktoken은 OpenAI에서 만들어진 encoder이다.\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"The Ministry of Love was the really frightening one. There were no windows in it at all. Winston had never been inside the Ministry of Love, nor within half a kilometre of it. It was a place impossible to enter except on official business, and then only by penetrating through a maze of barbed-wire entanglements, steel doors, and hidden machine-gun nests. Even the streets leading up to its outer barriers were roamed by gorilla-faced guards in black uniforms, armed with jointed truncheons.\\nWinston turned round abruptly. He had set his features into the expression of quiet optimism which it was advisable to wear when facing the telescreen. He crossed the room into the tiny kitchen. By leaving the Ministry at this time of day he had sacrificed his lunch in the canteen, and he was aware that there was no food in the kitchen except a hunk of dark-coloured bread which had got to be saved for tomorrow's breakfast. He took down from the shelf a bottle of colourless liquid with a plain white label marked VICTORY GIN. It gave off a sickly, oily smell, as of Chinese ricespirit. Winston poured out nearly a teacupful, nerved himself for a shock, and gulped it down like a dose of medicine.\\nInstantly his face turned scarlet and the water ran out of his eyes. The stuff was like nitric acid, and moreover, in swallowing it one had the sensation of being hit on the back of the head with a rubber club. The next moment, however, the burning in his belly died down and the world began to look more cheerful. He took a cigarette from a crumpled packet marked VICTORY CIGARETTES and incautiously held it upright, whereupon the tobacco fell out on to the floor. With the next he was more successful. He went back to the living-room and sat down at a small table that stood to the left of the telescreen. From the table drawer he took out a penholder, a bottle of ink, and a thick, quarto-sized blank book with a red back and a marbled cover.\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content='Part 1, Chapter 1\\nPart One\\n1 It was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.\\nThe hallway smelt of boiled cabbage and old rag mats. At one end of it a coloured poster, too large for indoor display, had been tacked to the wall. It depicted simply an enormous face, more than a metre wide: the face of a man of about forty-five, with a heavy black moustache and ruggedly handsome features. Winston made for the stairs. It was no use trying the lift. Even at the best of times it was seldom working, and at present the electric current was cut off during daylight hours. It was part of the economy drive in preparation for Hate Week. The flat was seven flights up, and Winston, who was thirty-nine and had a varicose ulcer above his right ankle, went slowly, resting several times on the way. On each landing, opposite the lift-shaft, the poster with the enormous face gazed from the wall. It was one of those pictures which are so contrived that the eyes follow you about when you move. BIG BROTHER IS WATCHING YOU, the caption beneath it ran.\\nInside the flat a fruity voice was reading out a list of figures which had something to do with the production of pig-iron. The voice came from an oblong metal plaque like a dulled mirror which formed part of the surface of the right-hand wall. Winston turned a switch and the voice sank somewhat, though the words were still distinguishable. The instrument (the telescreen, it was called) could be dimmed, but there was no way of shutting it off completely. He moved over to the window: a smallish, frail figure, the meagreness of his body merely emphasized by the blue overalls which were the uniform of the party. His hair was very fair, his face naturally sanguine, his skin roughened by coarse soap and blunt razor blades and the cold of the winter that had just ended.', metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"Outside, even through the shut window-pane, the world looked cold. Down in the street little eddies of wind were whirling dust and torn paper into spirals, and though the sun was shining and the sky a harsh blue, there seemed to be no colour in anything, except the posters that were plastered everywhere. The blackmoustachio'd face gazed down from every commanding corner. There was one on the house-front immediately opposite. BIG BROTHER IS WATCHING YOU, the caption said, while the dark eyes looked deep into Winston's own. Down at streetlevel another poster, torn at one corner, flapped fitfully in the wind, alternately covering and uncovering the single word INGSOC. In the far distance a helicopter skimmed down between the roofs, hovered for an instant like a bluebottle, and darted away again with a curving flight. It was the police patrol, snooping into people's windows. The patrols did not matter, however. Only the Thought Police mattered.\\nBehind Winston's back the voice from the telescreen was still babbling away about pig-iron and the overfulfilment of the Ninth Three-Year Plan. The telescreen received and transmitted simultaneously. Any sound that Winston made, above the level of a very low whisper, would be picked up by it, moreover, so long as he remained within the field of vision which the metal plaque commanded, he could be seen as well as heard. There was of course no way of knowing whether you were being watched at any given moment. How often, or on what system, the Thought Police plugged in on any individual wire was guesswork. It was even conceivable that they watched everybody all the time. But at any rate they could plug in your wire whenever they wanted to. You had to live -- did live, from habit that became instinct -- in the assumption that every sound you made was overheard, and, except in darkness, every movement scrutinized.\", metadata={'source': './files/chapter_one.txt'}),\n",
       " Document(page_content=\"For some reason the telescreen in the living-room was in an unusual position. Instead of being placed, as was normal, in the end wall, where it could command the whole room, it was in the longer wall, opposite the window. To one side of it there was a shallow alcove in which Winston was now sitting, and which, when the flats were built, had probably been intended to hold bookshelves. By sitting in the alcove, and keeping well back, Winston was able to remain outside the range of the telescreen, so far as sight went. He could be heard, of course, but so long as he stayed in his present position he could not be seen. It was partly the unusual geography of the room that had suggested to him the thing that he was now about to do.\\nBut it had also been suggested by the book that he had just taken out of the drawer. It was a peculiarly beautiful book. Its smooth creamy paper, a little yellowed by age, was of a kind that had not been manufactured for at least forty years past. He could guess, however, that the book was much older than that. He had seen it lying in the window of a frowsy little junk-shop in a slummy quarter of the town (just what quarter he did not now remember) and had been stricken immediately by an overwhelming desire to possess it. Party members were supposed not to go into ordinary shops ('dealing on the free market', it was called), but the rule was not strictly kept, because there were various things, such as shoelaces and razor blades, which it was impossible to get hold of in any other way. He had given a quick glance up and down the street and then had slipped inside and bought the book for two dollars fifty. At the time he was not conscious of wanting it for any particular purpose. He had carried it guiltily home in his briefcase. Even with nothing written in it, it was a compromising possession.\", metadata={'source': './files/chapter_one.txt'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력 내용과 유사한 chunk 들을 vectorDB에서 불러온다\n",
    "results = vectorstore.similarity_search(\"where does winston live\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.5 Langsmith (01:48)\n",
    "\n",
    "# https://www.langchain.com/langsmith\n",
    "# .env\n",
    "# LANGCHAIN_TRACING_V2 = True\n",
    "# LANGCHAIN_ENDPOINT = \"https://api.smith.langchain.com\"\n",
    "# LANGCHAIN_API_KEY = \"~Key~\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Victory Mansions is the name of the building where Winston Smith resides. It\\'s a run-down apartment complex located in London, the chief city of Airstrip One. The building is described as having a faulty elevator, with the electric current being cut off during daylight hours as part of an economy drive. The hallway of Victory Mansions smells of boiled cabbage and old rag mats. The apartment complex is described as having a gritty, dusty entrance, and Winston\\'s flat is located on the seventh floor. At one end of the hallway, there\\'s a large colored poster depicting the face of a man with the caption \"BIG BROTHER IS WATCHING YOU.\" The building is grimy and dilapidated, with a meagre and worn-down appearance.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6.6 RetrievalQA (18:22)\n",
    "# LangChain Expression Language(LCEL)\n",
    "# off-the-shelf chain은 커스텀이 어렵고 업데이트에 따라 소실될 수도 있다\n",
    "# Stuff(채우기) <- 찾은 문서를 토대로 프롬프트를 채워 사용하는 방안\n",
    "# Refine(정제, 다듬기) <- model로 질문과 문서를 프롬프트형태로 보낸 후, 받아낸 답변을 다시 문서와 함께 프롬프트형태로 model에 보내 답변을 얻는 과정을 반복하는것\n",
    "# 즉, Refine의 경우 문서 10개를 참고할 경우 model로 10번 반복해서 답변 생성을 요청하게 된다.\n",
    "# Map reduce documents chain <- query에 따른 참고 문서들 각각을 model을 사용하여 모두 요약하는 과정 수행. 이후 요약 내용을 종합하여 프롬프트를 구성하여 model에 답변을 요구함. 경우에 따라선, 요약이 아닌 발췌 방식으로 진행되기도 하며, 이러한 발췌는 질문과 관련된 부분을 중심으로 진행됨.\n",
    "# Map re-rank documents chain <- 각 document들을 사용한 대한 각각의 프롬프트를 구성하여 model로부터 답변을 요구. 받아낸 각 답변에 대해 점수를 매긴 후 가장 높은 점수를 기록한 답변을 최종 반환. 여기서 점수 매기기의 기준과 예시를 프롬프트에 포함시킴으로서 model이 수행하도록 요구함. output parser를 사용하여 최고점수를 받은 것을 반환하도록함\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "# Retrieval란, 구조화되지 않은 쿼리를 해석하여, 문서들을 반환\n",
    "# VectorStore는 그러한 탐색 과정에서 핵심 역할을 수행할 수 있으나 그게 필요하지 않은 경우 또한 존재하므로 필수는 아니라고 할 수 있다.\n",
    "# chain_type 의 기본 설정은 stuff\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    # vectorstore에서 as_retriever을 통해 retriever로의 사용이 가능하도록 한다.\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")\n",
    "\n",
    "chain.run(\"Describe Victory Mansions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이유는 불분명하지만, FAISS와 Chroma VectorStore 중 FAISS의 성능이 더 우수하다고 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Victory Mansions is a building where Winston Smith lives. It is described as having glass doors that let in gritty dust, a hallway that smells of boiled cabbage and old rag mats, and a poster of a large face with the caption \"BIG BROTHER IS WATCHING YOU.\" The building has a faulty lift, and Winston, who is thirty-nine with a varicose ulcer, lives on the seventh floor. The building is part of the city of London in Airstrip One, a province of Oceania.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6.7~6.8 Stuff LCEL Chain (10:13)\n",
    "# LCEL 구조 docs: https://python.langchain.com/v0.1/docs/expression_language/interface/\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# retriver: query string -> list of chunk. 이건 context 와 같은 속성에 넣어 prompt에서 사용\n",
    "# prompt: template에 정의된 속성들 입력 -> 프롬프트\n",
    "# llm: 프롬프트 -> 답변\n",
    "chain = (\n",
    "    # RunnableParallel에 의해 해당 중괄호 내의 작업은 병렬로 처리된다. 즉, 많은 속성을 동시에 사용하더라도 해당 과정의 작업 속도가 크게 느려지진 않는다.\n",
    "    {\n",
    "        # 여기서 retriver은 query에 의한 chunk 탐색과정을 수행한 후 list를 반환함. retriver(query)가 생략된 것과 동일\n",
    "        \"context\": retriver,\n",
    "        # 질문을 전달받음. RunnablePassthrough은 chain의 메소드에 의해 전달된 인자를 반환해주는 동작을 하는 것으로 추측됨.\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    # prompt.format_messages(context=retriver(query),question=RunnablePassthrough())가 생략된 것과 동일.\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "chain.invoke(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#7 DocumentGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.0 Introduction (05:01)\n",
    "# Home.py 파일 <- streamlit run Home.py를 수행함으로써 UI 활성화\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"Hello world!\")\n",
    "\n",
    "st.subheader(\"Welcome to Streamlit!\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "    #### I love it!\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.1 Magic (06:31)\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "st.selectbox(\n",
    "    \"Choose your model\",\n",
    "    (\n",
    "        \"GPT-3\",\n",
    "        \"GPT-4\",\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.2 Data Flow (06:18)\n",
    "# 수정사항 발생할 때마다 모든 코드가 다시 실행되는 data flow를 지니고 있음\n",
    "# 단순히 selectbox를 변경할 때에도 수정사항으로 취급하여 전체 코드가 재실행됨\n",
    "# 물론, Streamlit 내부에서 기본적으로 재실행되지 않는 부분 또한 존재\n",
    "import streamlit as st\n",
    "from datetime import datetime\n",
    "\n",
    "# 이 코드는, 해당 코드를 실행시킨 순간의 시간을 변수에 string으로 담는 것이기에, 시간이 지남에 따른 자동 업데이트 기능은 없다.\n",
    "# 단순히 코드 재실행 여부를 알려주기 위해 존재하는 코드이다.\n",
    "today = datetime.today().strftime(\"%H:%M:%S\")\n",
    "\n",
    "st.title(today)\n",
    "\n",
    "\n",
    "model = st.selectbox(\n",
    "    \"Choose your model\",\n",
    "    (\n",
    "        \"GPT-3\",\n",
    "        \"GPT-4\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "if model == \"GPT-3\":\n",
    "    st.write(\"cheap\")\n",
    "else:\n",
    "    st.write(\"not cheap\")\n",
    "    name = st.text_input(\"What is your name?\")\n",
    "    st.write(name)\n",
    "\n",
    "    value = st.slider(\n",
    "        \"temperature\",\n",
    "        min_value=0.1,\n",
    "        max_value=1.0,\n",
    "    )\n",
    "\n",
    "    st.write(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.3 Multi Page\n",
    "\n",
    "# Home.py\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"FullstackGPT Home\",\n",
    "    page_icon=\"🤖\",\n",
    ")\n",
    "\n",
    "# 여기서 /DocumentGPT 은 pages/01_DocumentGPT.py 로 이동시킨다.\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "# Hello!\n",
    "            \n",
    "Welcome to my FullstackGPT Portfolio!\n",
    "            \n",
    "Here are the apps I made:\n",
    "            \n",
    "- [ ] [DocumentGPT](/DocumentGPT)\n",
    "- [ ] [PrivateGPT](/PrivateGPT)\n",
    "- [ ] [QuizGPT](/QuizGPT)\n",
    "- [ ] [SiteGPT](/SiteGPT)\n",
    "- [ ] [MeetingGPT](/MeetingGPT)\n",
    "- [ ] [InvestorGPT](/InvestorGPT)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages/01_DocumentGPT.py\n",
    "\n",
    "# 여기서 01_은 sidebar에서의 순서를 위해 지정한 것. sidebar에서 해당 숫자가 보이진 않음\n",
    "# 페이지 네비게이션를 위해선 pages 폴더가 필수 \n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"DocumentGPT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.4 Chat Messages\n",
    "\n",
    "# pages/01_DocumentGPT.py\n",
    "import time\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"DocumentGPT\",\n",
    "    page_icon=\"📃\",\n",
    ")\n",
    "\n",
    "st.title(\"DocumentGPT\")\n",
    "\n",
    "# 대화 내용을 저장할 공간 마련\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = []\n",
    "\n",
    "\n",
    "def send_message(message, role, save=True):\n",
    "    with st.chat_message(role):\n",
    "        st.write(message)\n",
    "    if save:\n",
    "        # 대화 내용 저장\n",
    "        st.session_state[\"messages\"].append({\"message\": message, \"role\": role})\n",
    "\n",
    "\n",
    "# 대화 내용 전부 불러오기\n",
    "for message in st.session_state[\"messages\"]:\n",
    "    send_message(\n",
    "        message[\"message\"],\n",
    "        message[\"role\"],\n",
    "        save=False,\n",
    "    )\n",
    "\n",
    "\n",
    "message = st.chat_input(\"Send a message to the ai \")\n",
    "\n",
    "if message:\n",
    "    send_message(message, \"human\")\n",
    "    time.sleep(2)\n",
    "    send_message(f\"You said: {message}\", \"ai\")\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.write(st.session_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.6 Uploading Documents \n",
    "# cache를 사용해서 업로드한 documents에 대한 기록과 임베딩 정보를 기록\n",
    "# .cache 폴더 내에 files과 embeddings 폴더를 생성해둘 것\n",
    "\n",
    "# pages/01_DocumentGPT.py\n",
    "import time\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"DocumentGPT\",\n",
    "    page_icon=\"📃\",\n",
    ")\n",
    "\n",
    "\n",
    "def embed_file(file):\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/files/{file.name}\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "    cache_dir = LocalFileStore(f\"./.cache/embeddings/{file.name}\")\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100, \n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "    vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "\n",
    "st.title(\"DocumentGPT\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "Welcome!\n",
    "            \n",
    "Use this chatbot to ask questions to an AI about your files!\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "file = st.file_uploader(\n",
    "    \"Upload a .txt .pdf or .docx file\",\n",
    "    type=[\"pdf\", \"txt\", \"docx\"],\n",
    ")\n",
    "\n",
    "if file:\n",
    "    retriever = embed_file(file)\n",
    "    s = retriever.invoke(\"winston\")\n",
    "    s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamlit에서 환경변수 설정하기\n",
    "# .streamlit/secrets.toml\n",
    "\n",
    "# OPENAI_API_KEY=\"~key~\"\n",
    "\n",
    "# LANGCHAIN_TRACING_V2 = true\n",
    "# LANGCHAIN_ENDPOINT = \"https://api.smith.langchain.com\"\n",
    "# LANGCHAIN_API_KEY = \"~key~\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.7 Chat History\n",
    "\n",
    "\n",
    "# pages/01_DocumentGPT.py\n",
    "import time\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"DocumentGPT\",\n",
    "    page_icon=\"📃\",\n",
    ")\n",
    "\n",
    "# @st.cache_data(show_spinner=\"Embedding file...\")을 달 경우,\n",
    "# 함수에 넘긴 input이 이전에 넘겼던 input과 동일할 경우, 완전히 동일한 함수를 재실행하는 것으로 판단하여\n",
    "# Streamlit은 해당 함수가 그 input을 를 받았을 때 반환했던 값을 cach에서 가져와서 그대로 반환한 후 종료한다.\n",
    "# 여기서 input 동일성 여부를 판단함에 있어서, hash를 사용한다.\n",
    "@st.cache_data(show_spinner=\"Embedding file...\")\n",
    "def embed_file(file):\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/files/{file.name}\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "    cache_dir = LocalFileStore(f\"./.cache/embeddings/{file.name}\")\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "    vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def send_message(message, role, save=True):\n",
    "    with st.chat_message(role):\n",
    "        st.markdown(message)\n",
    "    if save:\n",
    "        st.session_state[\"messages\"].append({\"message\": message, \"role\": role})\n",
    "\n",
    "\n",
    "def paint_history():\n",
    "    for message in st.session_state[\"messages\"]:\n",
    "        send_message(\n",
    "            message[\"message\"],\n",
    "            message[\"role\"],\n",
    "            save=False,\n",
    "        )\n",
    "\n",
    "\n",
    "st.title(\"DocumentGPT\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "Welcome!\n",
    "            \n",
    "Use this chatbot to ask questions to an AI about your files!\n",
    "\n",
    "Upload your files on the sidebar.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "with st.sidebar:\n",
    "    file = st.file_uploader(\n",
    "        \"Upload a .txt .pdf or .docx file\",\n",
    "        type=[\"pdf\", \"txt\", \"docx\"],\n",
    "    )\n",
    "\n",
    "if file:\n",
    "    retriever = embed_file(file)\n",
    "    send_message(\"I'm ready! Ask away!\", \"ai\", save=False)\n",
    "    paint_history()\n",
    "    message = st.chat_input(\"Ask anything about your file...\")\n",
    "    if message:\n",
    "        send_message(message, \"human\")\n",
    "        send_message(f\"your messages: {message}\", \"ai\")\n",
    "else:\n",
    "    st.session_state[\"messages\"] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.8 Chain\n",
    "\n",
    "\n",
    "# pages/01_DocumentGPT.py\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"DocumentGPT\",\n",
    "    page_icon=\"📃\",\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "@st.cache_data(show_spinner=\"Embedding file...\")\n",
    "def embed_file(file):\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/files/{file.name}\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "    cache_dir = LocalFileStore(f\"./.cache/embeddings/{file.name}\")\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "    vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def send_message(message, role, save=True):\n",
    "    with st.chat_message(role):\n",
    "        st.markdown(message)\n",
    "    if save:\n",
    "        st.session_state[\"messages\"].append({\"message\": message, \"role\": role})\n",
    "\n",
    "\n",
    "def paint_history():\n",
    "    for message in st.session_state[\"messages\"]:\n",
    "        send_message(\n",
    "            message[\"message\"],\n",
    "            message[\"role\"],\n",
    "            save=False,\n",
    "        )\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(document.page_content for document in docs)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Answer the question using ONLY the following context. If you don't know the answer just say you don't know. DON'T make anything up.\n",
    "            \n",
    "            Context: {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "st.title(\"DocumentGPT\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "Welcome!\n",
    "            \n",
    "Use this chatbot to ask questions to an AI about your files!\n",
    "\n",
    "Upload your files on the sidebar.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "with st.sidebar:\n",
    "    file = st.file_uploader(\n",
    "        \"Upload a .txt .pdf or .docx file\",\n",
    "        type=[\"pdf\", \"txt\", \"docx\"],\n",
    "    )\n",
    "\n",
    "# retriever | RunnableLambda(format_docs)은 format_docs(retriever.invoke(query))와 의미가 같다.\n",
    "if file:\n",
    "    retriever = embed_file(file)\n",
    "    send_message(\"I'm ready! Ask away!\", \"ai\", save=False)\n",
    "    paint_history()\n",
    "    message = st.chat_input(\"Ask anything about your file...\")\n",
    "    if message:\n",
    "        send_message(message, \"human\")\n",
    "        chain = (\n",
    "            {\n",
    "                \"context\": retriever | RunnableLambda(format_docs),\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | prompt\n",
    "            | llm\n",
    "        )\n",
    "        response = chain.invoke(message)\n",
    "        send_message(response.content, \"ai\")\n",
    "\n",
    "else:\n",
    "    st.session_state[\"messages\"] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.9 Streaming\n",
    "\n",
    "# ChatCallbackHandler 사용\n",
    "# pages/01_DocumentGPT.py\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"DocumentGPT\",\n",
    "    page_icon=\"📃\",\n",
    ")\n",
    "\n",
    "# 오래된 llm 모델에서는 간혹 지원하지 않을 수 있다.\n",
    "# 또한, Callback Handler는 event들을 listen하는 여러 function들을 가진다.\n",
    "class ChatCallbackHandler(BaseCallbackHandler):\n",
    "    message = \"\"\n",
    "\n",
    "    # args로 on_llm_start(1,2,3)등의 입력을 받을 수 있으며,\n",
    "    # kwargs로 on_llm_start(a=1, a=4, b=1)등의 입력을 받을 수 있다.\n",
    "    # llm을 초기화한다. message_box을 통해 스트리밍을 진행하고자한다.\n",
    "    def on_llm_start(self, *args, **kwargs):\n",
    "        # 생성 token에 대해 stream 형식으로 화면에 출력하기 위해 사용\n",
    "        self.message_box = st.empty()\n",
    "\n",
    "    # llm 답변을 저장한다.\n",
    "    def on_llm_end(self, *args, **kwargs):\n",
    "        save_message(self.message, \"ai\")\n",
    "\n",
    "    # 답변이 생성되는 과정(토큰 단위)에 따라 스트리밍된다.\n",
    "    def on_llm_new_token(self, token, *args, **kwargs):\n",
    "        self.message += token\n",
    "        self.message_box.markdown(self.message)\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        ChatCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "@st.cache_data(show_spinner=\"Embedding file...\")\n",
    "def embed_file(file):\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/files/{file.name}\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "    cache_dir = LocalFileStore(f\"./.cache/embeddings/{file.name}\")\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "    vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def save_message(message, role):\n",
    "    st.session_state[\"messages\"].append({\"message\": message, \"role\": role})\n",
    "\n",
    "\n",
    "def send_message(message, role, save=True):\n",
    "    with st.chat_message(role):\n",
    "        st.markdown(message)\n",
    "    if save:\n",
    "        save_message(message, role)\n",
    "\n",
    "\n",
    "def paint_history():\n",
    "    for message in st.session_state[\"messages\"]:\n",
    "        send_message(\n",
    "            message[\"message\"],\n",
    "            message[\"role\"],\n",
    "            save=False,\n",
    "        )\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(document.page_content for document in docs)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Answer the question using ONLY the following context. If you don't know the answer just say you don't know. DON'T make anything up.\n",
    "            \n",
    "            Context: {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "st.title(\"DocumentGPT\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "Welcome!\n",
    "            \n",
    "Use this chatbot to ask questions to an AI about your files!\n",
    "\n",
    "Upload your files on the sidebar.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "with st.sidebar:\n",
    "    file = st.file_uploader(\n",
    "        \"Upload a .txt .pdf or .docx file\",\n",
    "        type=[\"pdf\", \"txt\", \"docx\"],\n",
    "    )\n",
    "\n",
    "if file:\n",
    "    retriever = embed_file(file)\n",
    "    send_message(\"I'm ready! Ask away!\", \"ai\", save=False)\n",
    "    paint_history()\n",
    "    message = st.chat_input(\"Ask anything about your file...\")\n",
    "    if message:\n",
    "        send_message(message, \"human\")\n",
    "        chain = (\n",
    "            {\n",
    "                \"context\": retriever | RunnableLambda(format_docs),\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | prompt\n",
    "            | llm\n",
    "        )\n",
    "        # ai가 답변을 작성하고 있는 걸 표현하기 위해 사용\n",
    "        with st.chat_message(\"ai\"):\n",
    "            response = chain.invoke(message)\n",
    "\n",
    "\n",
    "else:\n",
    "    st.session_state[\"messages\"] = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.10 Recap\n",
    "\n",
    "# pages/01_DocumentGPT.py\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"DocumentGPT\",\n",
    "    page_icon=\"📃\",\n",
    ")\n",
    "\n",
    "\n",
    "class ChatCallbackHandler(BaseCallbackHandler):\n",
    "    message = \"\"\n",
    "\n",
    "    def on_llm_start(self, *args, **kwargs):\n",
    "        self.message_box = st.empty()\n",
    "\n",
    "    def on_llm_end(self, *args, **kwargs):\n",
    "        save_message(self.message, \"ai\")\n",
    "\n",
    "    def on_llm_new_token(self, token, *args, **kwargs):\n",
    "        self.message += token\n",
    "        self.message_box.markdown(self.message)\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        ChatCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "@st.cache_data(show_spinner=\"Embedding file...\")\n",
    "def embed_file(file):\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/files/{file.name}\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "    cache_dir = LocalFileStore(f\"./.cache/embeddings/{file.name}\")\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "    vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def save_message(message, role):\n",
    "    st.session_state[\"messages\"].append({\"message\": message, \"role\": role})\n",
    "\n",
    "\n",
    "def send_message(message, role, save=True):\n",
    "    with st.chat_message(role):\n",
    "        st.markdown(message)\n",
    "    if save:\n",
    "        save_message(message, role)\n",
    "\n",
    "\n",
    "def paint_history():\n",
    "    for message in st.session_state[\"messages\"]:\n",
    "        send_message(\n",
    "            message[\"message\"],\n",
    "            message[\"role\"],\n",
    "            save=False,\n",
    "        )\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(document.page_content for document in docs)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Answer the question using ONLY the following context. If you don't know the answer just say you don't know. DON'T make anything up.\n",
    "            \n",
    "            Context: {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "st.title(\"DocumentGPT\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "Welcome!\n",
    "            \n",
    "Use this chatbot to ask questions to an AI about your files!\n",
    "\n",
    "Upload your files on the sidebar.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "with st.sidebar:\n",
    "    file = st.file_uploader(\n",
    "        \"Upload a .txt .pdf or .docx file\",\n",
    "        type=[\"pdf\", \"txt\", \"docx\"],\n",
    "    )\n",
    "\n",
    "if file:\n",
    "    retriever = embed_file(file)\n",
    "    send_message(\"I'm ready! Ask away!\", \"ai\", save=False)\n",
    "    paint_history()\n",
    "    message = st.chat_input(\"Ask anything about your file...\")\n",
    "    if message:\n",
    "        send_message(message, \"human\")\n",
    "        chain = (\n",
    "            {\n",
    "                \"context\": retriever | RunnableLambda(format_docs),\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | prompt\n",
    "            | llm\n",
    "        )\n",
    "        with st.chat_message(\"ai\"):\n",
    "            chain.invoke(message)\n",
    "\n",
    "\n",
    "else:\n",
    "    st.session_state[\"messages\"] = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 코드에서 session_state가 아닌 memory으로 전환 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#8 PrivateGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.0 Introduction (04:14)\n",
    "\n",
    "# pages/02_PrivateGPT.py\n",
    "# .cache 폴더 내의 files과 embeddings 폴더가 겹칠 수 있으므로,\n",
    "# 서로 다른 embedding 모델을 사용했다는 걸 고려하여 구분 가능하도록 폴더를 추가한다.\n",
    "# ex: .cache/private_files, .cache/private_embeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"PrivateGPT\",\n",
    "    page_icon=\"📃\",\n",
    ")\n",
    "\n",
    "\n",
    "class ChatCallbackHandler(BaseCallbackHandler):\n",
    "    message = \"\"\n",
    "\n",
    "    def on_llm_start(self, *args, **kwargs):\n",
    "        self.message_box = st.empty()\n",
    "\n",
    "    def on_llm_end(self, *args, **kwargs):\n",
    "        save_message(self.message, \"ai\")\n",
    "\n",
    "    def on_llm_new_token(self, token, *args, **kwargs):\n",
    "        self.message += token\n",
    "        self.message_box.markdown(self.message)\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        ChatCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "@st.cache_data(show_spinner=\"Embedding file...\")\n",
    "def embed_file(file):\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/private_files/{file.name}\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "    cache_dir = LocalFileStore(f\"./.cache/private_embeddings/{file.name}\")\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "    vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def save_message(message, role):\n",
    "    st.session_state[\"messages\"].append({\"message\": message, \"role\": role})\n",
    "\n",
    "\n",
    "def send_message(message, role, save=True):\n",
    "    with st.chat_message(role):\n",
    "        st.markdown(message)\n",
    "    if save:\n",
    "        save_message(message, role)\n",
    "\n",
    "\n",
    "def paint_history():\n",
    "    for message in st.session_state[\"messages\"]:\n",
    "        send_message(\n",
    "            message[\"message\"],\n",
    "            message[\"role\"],\n",
    "            save=False,\n",
    "        )\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(document.page_content for document in docs)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Answer the question using ONLY the following context. If you don't know the answer just say you don't know. DON'T make anything up.\n",
    "            \n",
    "            Context: {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "st.title(\"PrivateGPT\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "Welcome!\n",
    "            \n",
    "Use this chatbot to ask questions to an AI about your files!\n",
    "\n",
    "Upload your files on the sidebar.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "with st.sidebar:\n",
    "    file = st.file_uploader(\n",
    "        \"Upload a .txt .pdf or .docx file\",\n",
    "        type=[\"pdf\", \"txt\", \"docx\"],\n",
    "    )\n",
    "\n",
    "if file:\n",
    "    retriever = embed_file(file)\n",
    "    send_message(\"I'm ready! Ask away!\", \"ai\", save=False)\n",
    "    paint_history()\n",
    "    message = st.chat_input(\"Ask anything about your file...\")\n",
    "    if message:\n",
    "        send_message(message, \"human\")\n",
    "        chain = (\n",
    "            {\n",
    "                \"context\": retriever | RunnableLambda(format_docs),\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | prompt\n",
    "            | llm\n",
    "        )\n",
    "        with st.chat_message(\"ai\"):\n",
    "            chain.invoke(message)\n",
    "\n",
    "\n",
    "else:\n",
    "    st.session_state[\"messages\"] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.1 HuggingFaceHub\n",
    "\n",
    "# model inference api 사용하기\n",
    "\n",
    "# Instruct model의 경우 template에 대한 구체적인 가이드라인이 정해져 있다. 이를 고려하여 API 요청을 보내는 것이 중요하다.\n",
    "# 이는 해당 모델을 직접 다운로드 받아 사용할 때에도 동일한 이슈일 것으로 판단된다.\n",
    "# 구체적인 관련 사항은 해당 모델을 업로드한 기업 홈페이지의 model documentation을 확인하는게 중요하다.\n",
    "# 또한, 허깅페이스에서 max new token 설정 범위 또한 알아야 한다.\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"[INST]What is the meaning of {word}[/INST]\")\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 250,\n",
    "    },\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"word\": \"potato\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.2 HuggingFacePipeline\n",
    "\n",
    "# model 다운 후 로컬에서 돌리기\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"A {word} is a\")\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",\n",
    "    # task는 text-generation, text2text-generation, summarization 등 존재. 허깅페이스 repo에 특화 task가 서술되어 있음\n",
    "    task=\"text-generation\",\n",
    "    # device=0로 설정 시에 GPU로 동작. device=-1로 설정 시에는 CPU로 동작.\n",
    "    pipeline_kwargs={\"max_new_tokens\": 150},\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"word\": \"tomato\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.3 GPT4All\n",
    "\n",
    "# 로컬에서 model 실행하는 또 다른 방법\n",
    "# 실제 로컬 모델과 함께 사용할 수 있는 UI 애플리케이션을 갖고 있음\n",
    "# fine tuning을 할 수 있는 많은 모델들을 유지하고 있음\n",
    "# GPT4All 홈페이지에서 모델을 다운받아와야 사용 가능\n",
    "from langchain.llms.gpt4all import GPT4All\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"You are a helpful assistant that defines words. Define this word: {word}.\"\n",
    ")\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=\"./falcon.bin\",\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"word\": \"tomato\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.4 Ollama\n",
    "\n",
    "# llm을 로컬적으로 사용하는 방식 \n",
    "# pages/02_PrivateGPT.py\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import CacheBackedEmbeddings, OllamaEmbeddings\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"PrivateGPT\",\n",
    "    page_icon=\"📃\",\n",
    ")\n",
    "\n",
    "\n",
    "class ChatCallbackHandler(BaseCallbackHandler):\n",
    "    message = \"\"\n",
    "\n",
    "    def on_llm_start(self, *args, **kwargs):\n",
    "        self.message_box = st.empty()\n",
    "\n",
    "    def on_llm_end(self, *args, **kwargs):\n",
    "        save_message(self.message, \"ai\")\n",
    "\n",
    "    def on_llm_new_token(self, token, *args, **kwargs):\n",
    "        self.message += token\n",
    "        self.message_box.markdown(self.message)\n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"mistral:latest\",\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        ChatCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "@st.cache_data(show_spinner=\"Embedding file...\")\n",
    "def embed_file(file):\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/private_files/{file.name}\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "    cache_dir = LocalFileStore(f\"./.cache/private_embeddings/{file.name}\")\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    embeddings = OllamaEmbeddings(model=\"mistral:latest\")\n",
    "    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "    vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def save_message(message, role):\n",
    "    st.session_state[\"messages\"].append({\"message\": message, \"role\": role})\n",
    "\n",
    "\n",
    "def send_message(message, role, save=True):\n",
    "    with st.chat_message(role):\n",
    "        st.markdown(message)\n",
    "    if save:\n",
    "        save_message(message, role)\n",
    "\n",
    "\n",
    "def paint_history():\n",
    "    for message in st.session_state[\"messages\"]:\n",
    "        send_message(\n",
    "            message[\"message\"],\n",
    "            message[\"role\"],\n",
    "            save=False,\n",
    "        )\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(document.page_content for document in docs)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question using ONLY the following context and not your training data. If you don't know the answer just say you don't know. DON'T make anything up.\n",
    "    \n",
    "    Context: {context}\n",
    "    Question:{question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "st.title(\"PrivateGPT\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "Welcome!\n",
    "            \n",
    "Use this chatbot to ask questions to an AI about your files!\n",
    "\n",
    "Upload your files on the sidebar.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "with st.sidebar:\n",
    "    file = st.file_uploader(\n",
    "        \"Upload a .txt .pdf or .docx file\",\n",
    "        type=[\"pdf\", \"txt\", \"docx\"],\n",
    "    )\n",
    "\n",
    "if file:\n",
    "    retriever = embed_file(file)\n",
    "    send_message(\"I'm ready! Ask away!\", \"ai\", save=False)\n",
    "    paint_history()\n",
    "    message = st.chat_input(\"Ask anything about your file...\")\n",
    "    if message:\n",
    "        send_message(message, \"human\")\n",
    "        chain = (\n",
    "            {\n",
    "                \"context\": retriever | RunnableLambda(format_docs),\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | prompt\n",
    "            | llm\n",
    "        )\n",
    "        with st.chat_message(\"ai\"):\n",
    "            chain.invoke(message)\n",
    "\n",
    "\n",
    "else:\n",
    "    st.session_state[\"messages\"] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
