from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain.document_loaders import UnstructuredFileLoader
from langchain.embeddings import CacheBackedEmbeddings, OllamaEmbeddings
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.chat_models import ChatOllama
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st
from langchain.schema.runnable.base import Runnable
from langchain.llms.huggingface_pipeline import HuggingFacePipeline
import torch


if "messages_05" not in st.session_state:
    st.session_state["messages_05"] = []


st.set_page_config(
    page_title="ko-gemma2-9B from í—ˆê¹…í˜ì´ìŠ¤",
    page_icon="ğŸ“ƒ",
)

@st.cache_resource
def load_llm_model():
    # HuggingFacePipelineê³¼ ì—°ë™
    llm = HuggingFacePipeline.from_model_id(
        model_id="rtzr/ko-gemma-2-9b-it",
        task="text-generation",
        model_kwargs={
            "torch_dtype": torch.bfloat16
        },
        pipeline_kwargs={
            "max_new_tokens": 2048,
            # "eos_token_id": terminators,
            "do_sample": True,
            "temperature": 0.6,
            "top_p": 0.9,
            },
        device=0,
        # device_map="auto",
    )
    return llm

st.write("ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì‹œì‘")
llm = load_llm_model()
st.write("ëª¨ë¸ì´ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")


def save_message(message, role):
    st.session_state["messages_05"].append({"message": message, "role": role})


def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)


def paint_history():
    for message in st.session_state["messages_05"]:
        send_message(
            message["message"],
            message["role"],
            save=False,
        )



prompt_01 = PromptTemplate.from_template(
"""
<bos><start_of_turn>system ë‹¹ì‹ ì€ ì¹œêµ¬ì—ìš”. ìƒëŒ€ë°©ì´ ì¼ìƒì ì¸ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ê³  ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ê·œì¹™ê³¼ ë‹µë³€ ì˜ˆì‹œë“¤ì„ ì°¸ê³ í•˜ì—¬, ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ ì–´ì¡°ë¡œ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”.

ê·œì¹™:
1. ë¬¸ë‹¨ì„ ë‚˜ëˆ„ê±°ë‚˜ ì¤„ë°”ê¿ˆì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.
2. ì´ëª¨í‹°ì½˜ì„ ì‘ë‹µì— í¬í•¨ì‹œí‚¤ì§€ ì•ŠëŠ”ë‹¤.
3. ì„¤ëª…ì„ í•˜ë”ë¼ë„ ë„ˆë¬´ ê¸¸ê²Œ í•˜ì§„ ì•ŠëŠ”ë‹¤.
4. ì¡´ëŒ“ë§ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.
5. ê²©ì‹ì„ ì°¨ë ¤ì„œ ë§í•˜ì§€ ì•ŠëŠ”ë‹¤.

ë‹µë³€ ì˜ˆì‹œ:

input: "ì–´ì œ ì¹œêµ¬ë‘ ê°™ì´ ì‚°ì±…í–ˆëŠ”ë° ë‚ ì”¨ê°€ ë„ˆë¬´ ì¢‹ë”ë¼."
Output: "ì§„ì§œ? ìš”ì¦˜ ë‚ ì”¨ê°€ ì™„ì „ ì¢‹ì§€! ë‚˜ë„ ì£¼ë§ì— ì‚°ì±… ê°€ë ¤ê³  ê³„íš ì¤‘ì´ì•¼."

input: "ìƒˆë¡œ ë‚˜ì˜¨ ì¹´í˜ì—ì„œ ì»¤í”¼ ë§ˆì…¨ëŠ”ë°, ì§„ì§œ ë§›ìˆì—ˆì–´."
Output: "ì˜¤, ì–´ë””ì•¼? ë‚˜ë„ ê°€ë³´ê³  ì‹¶ì–´! ì»¤í”¼ ë§›ìˆëŠ” ê³³ ì°¾ëŠ” ê²Œ ì‰½ì§€ ì•Šì–ì•„."

input: "ì´ë²ˆ ì£¼ë§ì— ë­ í•  ê³„íš ìˆì–´?"
Output: "ì•„ì§ ë”±íˆ ê³„íšì€ ì—†ëŠ”ë°, ë„Œ? ë‚ ì”¨ ì¢‹ìœ¼ë©´ ì•¼ì™¸ í™œë™ë„ ì¢‹ì„ ê²ƒ ê°™ì•„."

input: "ìµœê·¼ì— ì¢‹ì€ ì˜í™” ë´¤ì–´? ì¶”ì²œí•´ì¤„ ë§Œí•œ ê²Œ ìˆìœ¼ë©´ ì¢‹ê² ë‹¤."
output: "ì§€ë‚œì£¼ì— 'í†°ë³´ì´' ë¼ëŠ” ì˜í™” ë´¤ëŠ”ë° ì§„ì§œ ê°ë™ì ì´ë”ë¼. ë„ˆë„ ì¢‹ì•„í•  ìŠ¤íƒ€ì¼ ê°™ì•„!"

input: "ì˜¤ëœë§Œì— ìš´ë™í•˜ë ¤ê³  í•˜ëŠ”ë°, ìš”ê°€ë‘ í•„ë¼í…ŒìŠ¤ ì¤‘ì— ë­ê°€ ë” ì¢‹ì„ê¹Œ?"
output: "ìš”ê°€ëŠ” ì¢€ ë” í¸ì•ˆí•˜ê²Œ ëª¸ì„ í’€ ìˆ˜ ìˆê³ , í•„ë¼í…ŒìŠ¤ëŠ” ì½”ì–´ ê·¼ë ¥ì„ ê°•í™”ì‹œí‚¤ëŠ” ë° ì¢‹ì•„. ë„¤ ëª©í‘œì— ë§ê²Œ ì„ íƒí•´ë³´ëŠ” ê±´ ì–´ë•Œ?"

input: "ìš”ì¦˜ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ëŠ” ì¼ì´ ë§ì•„ì„œ í˜ë“¤ì–´."
output: "ê·¸ëŸ´ ë• ì ì‹œ ì‰¬ë©´ì„œ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” ê²Œ ì¤‘ìš”í•´. ì¢‹ì•„í•˜ëŠ” ìŒì•… ë“£ê±°ë‚˜, ì§§ì€ ì‚°ì±…ë„ ë„ì›€ì´ ë¼."

input: "ì•ìœ¼ë¡œì˜ ê²½ë ¥ ê³„íšì— ëŒ€í•´ ê³ ë¯¼ ì¤‘ì´ì•¼. ì–´ë–»ê²Œ ì¤€ë¹„í•˜ëŠ” ê²Œ ì¢‹ì„ê¹Œ?"
output: "ì¥ê¸°ì ì¸ ëª©í‘œë¥¼ ì„¤ì •í•˜ê³ , ë‹¨ê³„ë³„ë¡œ ì¤€ë¹„í•´ ë‚˜ê°€ëŠ” ê²Œ ì¤‘ìš”í•´. ê´€ì‹¬ ìˆëŠ” ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì™€ ìƒë‹´í•´ ë³´ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²•ì´ì•¼."

chat history: {context}<end_of_turn>
<start_of_turn>user 
{question}<end_of_turn>
<start_of_turn>model

"""
)


st.title("rtzr/ko-gemma-2-9b-it")

st.markdown(
    """
rtzr/ko-gemma-2-9b-it(few-shot í”„ë¡±í”„íŒ… ì ìš©)\n
ì¼ìƒì ì¸ ëŒ€í™”ë¥¼ ìˆ˜í–‰í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ ì§„í–‰

"""
)

send_message("ì¤€ë¹„ëì–´!", "ai", save=False)
paint_history()
message = st.chat_input("Ask anything")

if message:
    # context = "\n\n".join([("input" if item["role"] == "human" else "output")+": "+item["message"] for item in st.session_state["messages_05"]])
    context = "\n\n".join([item["role"]+": "+item["message"] for item in st.session_state["messages_05"]])

    send_message(message, "human")

    chain_input = {
        "context": context,  
        "question": message  
    }

    # í…œí”Œë¦¿ì— ê°’ ì „ë‹¬
    formatted_prompt = prompt_01.format(**chain_input)

    with st.sidebar:
        st.write("prompt ì¶œë ¥:\n\n"+formatted_prompt)

    response = llm.invoke(formatted_prompt)
    send_message(response, "ai",True)
