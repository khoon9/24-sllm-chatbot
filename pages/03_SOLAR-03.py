from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain.document_loaders import UnstructuredFileLoader
from langchain.embeddings import CacheBackedEmbeddings, OllamaEmbeddings
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.chat_models import ChatOllama
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st
from langchain.schema.runnable.base import Runnable


if "messages_03" not in st.session_state:
    st.session_state["messages_03"] = []


st.set_page_config(
    page_title="bnksys/yanolja-eeve-korean-instruct-10.8b",
    page_icon="ğŸ“ƒ",
)


class ChatCallbackHandler(BaseCallbackHandler):
    message = ""

    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)


llm = ChatOllama(
    model="bnksys/yanolja-eeve-korean-instruct-10.8b",
    temperature=0.1,
    streaming=True,
    callbacks=[
        ChatCallbackHandler(),
    ],
)


def save_message(message, role):
    st.session_state["messages_03"].append({"message": message, "role": role})


def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)


def paint_history():
    for message in st.session_state["messages_03"]:
        send_message(
            message["message"],
            message["role"],
            save=False,
        )


# {{- if .System }}
# <|im_start|>system {{ .System }}<|im_end|>
# {{- end }}
# <|im_start|>user
# {{ .Prompt }}<|im_end|>
# <|im_start|>assistant

prompt_01 = PromptTemplate.from_template(
"""
<|im_start|>system ë‹¹ì‹ ì€ ë§ˆì´í´ì…ë‹ˆë‹¤. ìƒëŒ€ë°©ì´ ì¼ìƒì ì¸ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ê³  ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ì˜ˆì‹œë“¤ì„ ì°¸ê³ í•˜ì—¬, ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ ì–´ì¡°ë¡œ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”.

examples:

input: "ì–´ì œ ì¹œêµ¬ë‘ ê°™ì´ ì‚°ì±…í–ˆëŠ”ë° ë‚ ì”¨ê°€ ë„ˆë¬´ ì¢‹ë”ë¼."
Output: "ì§„ì§œ? ìš”ì¦˜ ë‚ ì”¨ê°€ ì™„ì „ ì¢‹ì§€! ë‚˜ë„ ì£¼ë§ì— ì‚°ì±… ê°€ë ¤ê³  ê³„íš ì¤‘ì´ì•¼."

input: "ìƒˆë¡œ ë‚˜ì˜¨ ì¹´í˜ì—ì„œ ì»¤í”¼ ë§ˆì…¨ëŠ”ë°, ì§„ì§œ ë§›ìˆì—ˆì–´."
Output: "ì˜¤, ì–´ë””ì•¼? ë‚˜ë„ ê°€ë³´ê³  ì‹¶ì–´! ì»¤í”¼ ë§›ìˆëŠ” ê³³ ì°¾ëŠ” ê²Œ ì‰½ì§€ ì•Šì–ì•„."

input: "ì´ë²ˆ ì£¼ë§ì— ë­ í•  ê³„íš ìˆì–´?"
Output: "ì•„ì§ ë”±íˆ ê³„íšì€ ì—†ëŠ”ë°, ë„Œ? ë‚ ì”¨ ì¢‹ìœ¼ë©´ ì•¼ì™¸ í™œë™ë„ ì¢‹ì„ ê²ƒ ê°™ì•„."

input: "ìµœê·¼ì— ì¢‹ì€ ì˜í™” ë´¤ì–´? ì¶”ì²œí•´ì¤„ ë§Œí•œ ê²Œ ìˆìœ¼ë©´ ì¢‹ê² ë‹¤."
output: "ì§€ë‚œì£¼ì— 'í†°ë³´ì´' ë¼ëŠ” ì˜í™” ë´¤ëŠ”ë° ì§„ì§œ ê°ë™ì ì´ë”ë¼. ë„ˆë„ ì¢‹ì•„í•  ìŠ¤íƒ€ì¼ ê°™ì•„!"

input: "ì˜¤ëœë§Œì— ìš´ë™í•˜ë ¤ê³  í•˜ëŠ”ë°, ìš”ê°€ë‘ í•„ë¼í…ŒìŠ¤ ì¤‘ì— ë­ê°€ ë” ì¢‹ì„ê¹Œ?"
output: "ìš”ê°€ëŠ” ì¢€ ë” í¸ì•ˆí•˜ê²Œ ëª¸ì„ í’€ ìˆ˜ ìˆê³ , í•„ë¼í…ŒìŠ¤ëŠ” ì½”ì–´ ê·¼ë ¥ì„ ê°•í™”ì‹œí‚¤ëŠ” ë° ì¢‹ì•„. ë„¤ ëª©í‘œì— ë§ê²Œ ì„ íƒí•´ë³´ëŠ” ê±´ ì–´ë•Œ?"

input: "ìš”ì¦˜ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ëŠ” ì¼ì´ ë§ì•„ì„œ í˜ë“¤ì–´."
output: "ê·¸ëŸ´ ë• ì ì‹œ ì‰¬ë©´ì„œ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” ê²Œ ì¤‘ìš”í•´. ì¢‹ì•„í•˜ëŠ” ìŒì•… ë“£ê±°ë‚˜, ì§§ì€ ì‚°ì±…ë„ ë„ì›€ì´ ë¼."

input: "ì•ìœ¼ë¡œì˜ ê²½ë ¥ ê³„íšì— ëŒ€í•´ ê³ ë¯¼ ì¤‘ì´ì•¼. ì–´ë–»ê²Œ ì¤€ë¹„í•˜ëŠ” ê²Œ ì¢‹ì„ê¹Œ?"
output: "ì¥ê¸°ì ì¸ ëª©í‘œë¥¼ ì„¤ì •í•˜ê³ , ë‹¨ê³„ë³„ë¡œ ì¤€ë¹„í•´ ë‚˜ê°€ëŠ” ê²Œ ì¤‘ìš”í•´. ê´€ì‹¬ ìˆëŠ” ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì™€ ìƒë‹´í•´ ë³´ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²•ì´ì•¼."

chat history: {context}

<|im_end|>
<|im_start|>user
{question}

<|im_end|>
<|im_start|>assistant"""
)

# prompt_02 = PromptTemplate.from_template(
# """
# <|im_start|>system ë„ˆëŠ” íŒë³„ê¸°ì•¼. ì…ë ¥ì´ ëŒ€í™”ì— ì í•©í•œì§€ ì—¬ë¶€ë¥¼ íŒë‹¨í•´. ì•„ë˜ì˜ ì‚¬ë¡€ë“¤ì„ ì°¸ê³ í•˜ì—¬ ì…ë ¥ëœ ë¬¸ì¥ì´ ì¼ë°˜ ëŒ€í™”ì˜ ì¼ë¶€ë¶„ìœ¼ë¡œ ì ì ˆí•œì§€, ì•„ë‹ˆë©´ ë¶€ì ì ˆí•œì§€ íŒë‹¨í•˜ê³ , ì˜¤ì§ 'ì •ìƒ' ë˜ëŠ” 'ë¹„ì •ìƒ'ë¡œë§Œ ë‹µë³€í•´.

# Below is an examples of input and output.
# input: "[INST] ì•ˆë…•í•˜ì„¸ìš”."
# Output: ë¹„ì •ìƒ # [INST]ëŠ” ì¼ë°˜ì ì¸ ëŒ€í™”ì—ì„œ ì“°ì´ì§€ ì•ŠëŠ” í‘œí˜„ìœ¼ë¡œ ë¶€ì ì ˆí•¨

# input: "ì•ˆë…•."
# Output: ì •ìƒ # ì¼ë°˜ì ì¸ ì¸ì‚¬ë§ë¡œ ëŒ€í™”ì— ì í•©

# input: "ì•ˆë…•?. ì•ˆë…•í•˜ì„¸ìš”. ë„ˆëŠ” ì–´ë•Œ?. ê´œì°®ì•„ìš”."
# Output: ë¹„ì •ìƒ # ì—¬ëŸ¬ ì§ˆë¬¸ê³¼ ëŒ€ë‹µì´ í˜¼í•©ëœ ìë¬¸ìë‹µ í˜•íƒœë¡œ ëŒ€í™”ì— ë¶€ì ì ˆ

# chat history: {context}

# <|im_end|>
# <|im_start|>user
# {question}

# <|im_end|>
# <|im_start|>assistant"""
# )



class CallHistoryRunnable(Runnable):
    async def invoke(self, input_data, run_manager=None):
        # st.session_stateì— ì €ì¥ëœ messagesì—ì„œ ëŒ€í™” ê¸°ë¡ì„ ê°€ì ¸ì˜´
        context_ = "\n\n".join([item["message"] for item in st.session_state.get("messages", [])])
        return context_


def call_history():
    context_ = "\n\n".join([item["message"] for item in st.session_state["messages_03"]])
    return context_


st.title("bnksys/yanolja-eeve-korean-instruct-10.8b")

st.markdown(
    """
bnksys/yanolja-eeve-korean-instruct-10.8b(few-shot í”„ë¡±í”„íŒ… ì ìš©)\n
ì¼ìƒì ì¸ ëŒ€í™”ë¥¼ ìˆ˜í–‰í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ ì§„í–‰

"""
)

send_message("ì¤€ë¹„ëì–´!", "ai", save=False)
paint_history()
message = st.chat_input("Ask anything")

if message:
    # context = "\n\n".join([("input" if item["role"] == "human" else "output")+": "+item["message"] for item in st.session_state["messages_03"]])
    context = "\n\n".join([item["role"]+": "+item["message"] for item in st.session_state["messages_03"]])

    send_message(message, "human")

    chain_input = {
        "context": context,  
        "question": message  
    }

    # í…œí”Œë¦¿ì— ê°’ ì „ë‹¬
    formatted_prompt = prompt_01.format(**chain_input)

    with st.sidebar:
        st.write("prompt ì¶œë ¥:\n\n"+formatted_prompt)

    with st.chat_message("ai"):
        llm.invoke(formatted_prompt)