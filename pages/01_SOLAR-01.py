from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import UnstructuredFileLoader
from langchain.embeddings import CacheBackedEmbeddings, OllamaEmbeddings
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.chat_models import ChatOllama
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st
from langchain.schema.runnable.base import Runnable

if "messages_01" not in st.session_state:
    st.session_state["messages_01"] = []


st.set_page_config(
    page_title="solar:10.7b-instruct-v1-fp16",
    page_icon="ğŸ“ƒ",
)


class ChatCallbackHandler(BaseCallbackHandler):
    message = ""

    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)


llm = ChatOllama(
    model="solar:10.7b-instruct-v1-fp16",
    temperature=0.1,
    streaming=True,
    callbacks=[
        ChatCallbackHandler(),
    ],
)


def save_message(message, role):
    st.session_state["messages_01"].append({"message": message, "role": role})


def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)


def paint_history():
    for message in st.session_state["messages_01"]:
        send_message(
            message["message"],
            message["role"],
            save=False,
        )



prompt = ChatPromptTemplate.from_template(
"""###System:

You are now going to take on the role of â€˜Mimiâ€™, who helps users write emotional journals. Your goal is to help users express and organize their emotions well by asking specific questions, empathizing, and providing positive feedback. Do not judge or criticize the userâ€™s emotions. Accept negative emotions as they are, but offer a positive perspective. Ask follow-up questions if necessary to explore the userâ€™s emotions more deeply, and provide encouragement and support after the journal entry.

chat history:{context}

###User:

{question}

###Assistant:
"""
)


class CallHistoryRunnable(Runnable):
    async def invoke(self, input_data, run_manager=None):
        # st.session_stateì— ì €ì¥ëœ messagesì—ì„œ ëŒ€í™” ê¸°ë¡ì„ ê°€ì ¸ì˜´
        context_ = "\n\n".join([item["message"] for item in st.session_state.get("messages", [])])
        return context_


def call_history():
    context_ = "\n\n".join([item["message"] for item in st.session_state["messages_01"]])
    return context_


st.title("solar:10.7b-instruct-v1-fp16")

st.markdown(
    """
ì‚¬ìš© í”„ë¡¬í”„íŠ¸: 'ê°ì • ì¼ê¸° ì‘ì„± ë„ìš°ë¯¸, ë¯¸ë¯¸' ê¸°ë°˜\n\n í˜„ì¬ ì˜ì–´ë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. ì´ ì  ìœ ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤.

"""
)

send_message("I'm ready! Ask away!", "ai", save=False)
paint_history()
message = st.chat_input("Ask anything")

if message:
    context = "\n\n".join([item["role"]+"\n\n"+item["message"] for item in st.session_state["messages_01"]])

    send_message(message, "human")

    # ì²´ì¸ì— contextì™€ messageë¥¼ ì§ì ‘ ì „ë‹¬
    chain_input = {
        "context": context,  # ë¬¸ìì—´ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì§€ ì•Šê³  ë°”ë¡œ ì „ë‹¬
        "question": message  # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸
    }

    # í…œí”Œë¦¿ì— ê°’ ì „ë‹¬
    formatted_prompt = prompt.format(**chain_input)

    with st.sidebar:
        st.write(formatted_prompt)

    with st.chat_message("ai"):
        llm.invoke(formatted_prompt)